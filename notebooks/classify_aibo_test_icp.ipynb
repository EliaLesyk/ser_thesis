{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe16bdba-50af-46a9-8ae4-b29ea35cd3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from time import gmtime, strftime, time\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "# from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "# from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bb16c1-9288-4056-9605-bb936d882b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft_df = np.load(\"data/AIBO/w2v/\" + \"aibo_ft.npy\")\n",
    "# hs_df = np.load(\"/Users/el/embrace/data/data_wav2vec2/icp/\" + \"icp_hs.npy\")\n",
    "#labels_df = np.load(\"data/AIBO/w2v/\" + \"aibo_labels.npy\")\n",
    "\n",
    "# labels_dict = {'ang': 1, 'exc': 3, 'fea': 2, 'fru': 4, 'hap': 3, 'neu': 0, 'oth': 6, 'sad': 4, 'sur': 5, 'xxx': 6}\n",
    "labels_dict = {'A': 0, 'E': 1, 'N': 2, 'P': 3, 'R': 4}\n",
    "ID_TO_CLASS = {v: k for k, v in labels_dict.items()}\n",
    "w2v_classes = list(ID_TO_CLASS.keys())\n",
    "\n",
    "#lb_df = np.array([labels_dict[letter] for letter in labels_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453c5fd5-0993-4063-97d3-61842440af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa7880d-1657-4c1b-b0c3-ae00f7b892b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_option = 'xvec'\n",
    "#model_option = 'base'\n",
    "model_option = 'xlsr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2bde509-4470-4362-bf87-55c58141fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = os.getcwd()\n",
    "ICP_PATH = home_path + \"/data/IEMOCAP/wav2vec/\"\n",
    "AIBO_PATH = home_path + \"/data/AIBO/wav2vec/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73e8aab-b271-40d3-922e-d1290dadb78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15955,) (2261,)\n",
      "(15955, 3072) (2261, 3072)\n"
     ]
    }
   ],
   "source": [
    "#x_train = np.load(ICP_PATH + model_option + \"_icp_hs_train.npy\")\n",
    "#x_val = np.load(ICP_PATH + model_option + \"_icp_hs_val.npy\")\n",
    "\n",
    "#y_train = np.load(ICP_PATH + model + \"_icp_lb_train.npy\", allow_pickle=True)\n",
    "#y_val = np.load(ICP_PATH + model + \"_icp_lb_val.npy\", allow_pickle=True)\n",
    "\n",
    "x_train_trn = np.load(AIBO_PATH + model_option + \"_aibo_ft_train.npy\")\n",
    "x_train_tst = np.load(AIBO_PATH + model_option + \"_aibo_ft_test.npy\")\n",
    "x_train = np.vstack([x_train_trn, x_train_tst])\n",
    "x_val = np.load(AIBO_PATH + model_option + \"_aibo_ft_val.npy\")\n",
    "\n",
    "y_train_trn = np.load(AIBO_PATH + model_option + \"_aibo_lb_train.npy\", allow_pickle=True)\n",
    "y_train_tst = np.load(AIBO_PATH + model_option + \"_aibo_lb_test.npy\", allow_pickle=True)\n",
    "y_train = np.hstack([y_train_trn, y_train_tst])\n",
    "y_val = np.load(AIBO_PATH + model_option + \"_aibo_lb_val.npy\", allow_pickle=True)\n",
    "\n",
    "print(y_train.shape, y_val.shape)       \n",
    "print(x_train.shape, x_val.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32cd6602-10cf-48da-90e4-b69b47ab3222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15955,), (2261,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdaa0e4a-4cda-4a57-a3c3-42915627046e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18216,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.hstack((y_train, y_val))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bb5693e-c4a2-4ccd-aeac-772115c2ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4418, 1.0117, 0.3322, 4.0981, 2.8755])\n"
     ]
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights = t.tensor(class_weights, dtype=t.float)\n",
    " \n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7ab4df-c235-4479-b8a9-b9a626f662da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(t.tensor(x_train), t.from_numpy(y_train))\n",
    "# train_dataset = TensorDataset(torch.from_numpy(x_train_eval).float(), torch.from_numpy(y_train_eval).float())\n",
    "val_dataset = TensorDataset(t.tensor(x_val), t.from_numpy(y_val))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e21b476-b37a-4c82-b800-62dfc5e236f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tst = np.load(ICP_PATH + model_option + \"_icp_ft_test.npy\")\n",
    "x_test_trn = np.load(ICP_PATH + model_option + \"_icp_ft_train.npy\")\n",
    "x_test_vld = np.load(ICP_PATH + model_option + \"_icp_ft_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a290d259-d856-40f9-b031-6d73cf4fd9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7380, 3072)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.vstack([x_test_tst, x_test_trn, x_test_vld])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "769c87d6-fb1e-4a43-b72b-84285cd4245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tst = np.load(ICP_PATH + model_option + \"_icp_lb_test.npy\")\n",
    "y_test_trn = np.load(ICP_PATH + model_option + \"_icp_lb_train.npy\")\n",
    "y_test_vld = np.load(ICP_PATH + model_option + \"_icp_lb_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e39a6ab2-1e05-445e-b6f1-d325314b6ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7380,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.hstack([y_test_tst, y_test_trn, y_test_vld])\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a4c41e1-ba57-4c53-a052-aa080840bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(t.tensor(x_test), t.from_numpy(y_test))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74d3e8b7-1c46-4dc4-86b5-3fc14153e88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neu', 'hap', 'sad', 'fru', 'ang']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for classification report\n",
    "ICP_CLASS_TO_ID = {\"neu\": 0, \"hap\": 1, \"sad\": 2, \"fru\": 3, \"ang\": 4}\n",
    "CLASS_TO_ID = ICP_CLASS_TO_ID\n",
    "w2v_classnames = list(ICP_CLASS_TO_ID.keys())\n",
    "w2v_classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7993694b-259e-4e5d-a65a-4fc53f3f08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b36d38f6-071b-4e7d-8cf7-7a4a72f2b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a64f31b3-6757-454f-aef3-fa076dc3fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(y_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9a4edfa-e2e5-480e-82b0-4954bd187c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_x = ft_df\n",
    "#my_y = lb_df\n",
    "\n",
    "#split_index = int(my_y.shape[0]*0.8)\n",
    "#print(split_index)\n",
    "\n",
    "#all_indexes = list(range(my_x.shape[0]))\n",
    "#test_indexes = all_indexes[(split_index + 1):]\n",
    "#x_test = my_x[test_indexes]\n",
    "#y_test = my_y[test_indexes]\n",
    "\n",
    "#train_indexes = all_indexes[:(split_index + 1)]\n",
    "#x_train = my_x[train_indexes]\n",
    "#y_train = my_y[train_indexes]\n",
    "\n",
    "#eval_split_index = 934\n",
    "#all_indexes = list(range(x_train.shape[0]))\n",
    "#eval_indexes = all_indexes[(eval_split_index + 1):]\n",
    "#eval_train_indexes = all_indexes[:(eval_split_index + 1)]\n",
    "\n",
    "#x_train_eval = x_train[eval_indexes]\n",
    "#y_train_eval = y_train[eval_indexes]\n",
    "#x_eval = x_train[eval_train_indexes]\n",
    "#y_eval = y_train[eval_train_indexes]\n",
    "\n",
    "# x_train_eval=np.vstack(x_train_eval).astype(np.float)\n",
    "# y_train_eval=np.vstack(y_train_eval).astype(np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca952110-e687-4d1f-bc4e-baed2ce80523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train_eval.shape, y_eval.shape, y_test.shape)       # (4696,) (935,) (1407,)\n",
    "#print(x_train_eval.shape, x_eval.shape, x_test.shape)       # (4696, 3072) (935, 3072) (1407, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "016ab04a-7051-4953-a85d-43527e6fb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if t.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "875b24d9-d68b-4665-a6f3-999b745c93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,  # Model to be trained.\n",
    "                 model_name,\n",
    "                 dataset,\n",
    "                 crit,  # Loss function\n",
    "                 optim=None,  # Optimizer\n",
    "                 train_dl=None,  # Training data set\n",
    "                 val_test_dl=None,  # Validation data set\n",
    "                 test_dl=None,  # Test data set\n",
    "                 cuda=True,  # Whether to use the GPU\n",
    "                 early_stopping_patience=-1):   # The patience for early stopping\n",
    "                 #unsqueeze_needed=True\n",
    "        self._model = model\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self._crit = crit\n",
    "        self._optim = optim\n",
    "        self._train_dl = train_dl\n",
    "        self._val_test_dl = val_test_dl\n",
    "        self._test_dl = test_dl\n",
    "        self._cuda = cuda\n",
    "        self._early_stopping_patience = early_stopping_patience\n",
    "        #self._unsqueeze_needed = unsqueeze_needed\n",
    "\n",
    "        if cuda:\n",
    "            self._model = model.cuda()\n",
    "            self._crit = crit.cuda()\n",
    "\n",
    "    #def save_checkpoint(self, epoch):\n",
    "     #   t.save({'state_dict': self._model.state_dict()}, 'checkpoints/checkpoint_{:03d}.ckp'.format(epoch))\n",
    "\n",
    "    #def restore_checkpoint(self, epoch_n):\n",
    "    def restore_checkpoint(self):\n",
    "        path = 'checkpoints/' + self.model_name + '_checkpoint_{}.ckp'.format(get_datetime())\n",
    "        if os.path.exists(path):\n",
    "            #ckp = t.load('checkpoints/checkpoint_{:03d}.ckp'.format(epoch_n), 'cuda' if self._cuda else None)\n",
    "            ckp = t.load(path, 'cuda' if self._cuda else None)\n",
    "            self._model.load_state_dict(ckp['state_dict'])\n",
    "\n",
    "    def save_onnx(self, fn):\n",
    "        m = self._model.cpu()\n",
    "        m.eval()\n",
    "        x = t.randn(1, 3, 300, 300, requires_grad=True)\n",
    "        y = self._model(x)\n",
    "        t.onnx.export(m,  # model being run\n",
    "                      x,  # model input (or a tuple for multiple inputs)\n",
    "                      fn,  # where to save the model (can be a file or file-like object)\n",
    "                      export_params=True,  # store the trained parameter weights inside the model file\n",
    "                      opset_version=10,  # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names=['input'],  # the model's input names\n",
    "                      output_names=['output'],  # the model's output names\n",
    "                      dynamic_axes={'input': {0: 'batch_size'},  # variable lenght axes\n",
    "                                    'output': {0: 'batch_size'}})\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        # perform following steps:\n",
    "        # -reset the gradients / clear the gradients of all optimized variables\n",
    "        self._optim.zero_grad()\n",
    "        # -propagate through the network / forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = self._model.forward(x)\n",
    "        # -calculate the loss\n",
    "        #y=y.to(t.int64)\n",
    "        \n",
    "        \n",
    "        loss = self._crit(output, y)\n",
    "        # -compute gradient by backprop / backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # -update weights / perform a single optimization step (parameter update)\n",
    "        self._optim.step()\n",
    "        # -return the loss\n",
    "        return loss, output\n",
    "\n",
    "    def val_test_step(self, x, y):\n",
    "        # predict\n",
    "        # propagate through the network and calculate the loss and predictions\n",
    "        pred = self._model.forward(x)\n",
    "        # calculate the loss\n",
    "        #y=y.to(t.int64)\n",
    "        \n",
    "        loss = self._crit(pred, y)\n",
    "        # return the loss and the predictions\n",
    "        return loss, pred\n",
    "\n",
    "    def train_epoch(self):\n",
    "        # set training mode / prepare model for training\n",
    "        self._model.train()\n",
    "        # iterate through the training set\n",
    "        # clear lists to track next epoch\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for x, y in self._train_dl:\n",
    "            # transfer the batch to \"cuda()\" -> the gpu if a gpu is given\n",
    "            if self._cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "           # if self._unsqueeze_needed:\n",
    "            #    x = x.unsqueeze(1)\n",
    "            # perform a training step\n",
    "            #print(y.dtype)\n",
    "            loss, pred = self.train_step(x, y)\n",
    "            total_loss += loss.item()\n",
    "            #total_acc += accuracy_score(y.cpu().detach().numpy(), np.hstack(pred))\n",
    "            #total_acc += accuracy_score(y.cpu(), pred.cpu() > 0.5)\n",
    "        # calculate the average loss for the epoch and return it\n",
    "        total_loss = total_loss / len(self._train_dl)\n",
    "        #total_acc = total_acc / len(self._train_dl)\n",
    "        #print(\"Train: loss: {}, accuracy: {}\".format(total_loss, total_acc))\n",
    "        print(\"Train: loss: {}\".format(total_loss))\n",
    "        return total_loss\n",
    "\n",
    "    def val_test(self, mode=False):\n",
    "        # set eval mode / prepare model for evaluation\n",
    "        self._model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        # disable gradient computation (disable autograd engine)\n",
    "        with t.no_grad():\n",
    "            # iterate through the validation set\n",
    "            # clear lists to track next epoch\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            total_f1 = 0\n",
    "            if mode:\n",
    "                dataset = self._test_dl\n",
    "            else:\n",
    "                dataset = self._val_test_dl\n",
    "            for x, y in dataset:\n",
    "                # transfer the batch to the gpu if given\n",
    "                if self._cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                #if self._unsqueeze_needed:\n",
    "                 #   x = x.unsqueeze(1)\n",
    "\n",
    "                # perform a validation step / forward pass: compute predicted outputs by passing inputs to the model\n",
    "                loss, pred = self.val_test_step(x, y)       # pred.shape torch.Size([8, 5]) = bs, num_cl\n",
    "                # calculate metrics for this iteration\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # deal with multilabel\n",
    "                activation = t.nn.Softmax(dim=1)\n",
    "                pred = activation(pred.data)\n",
    "                pred = t.max(pred, 1)[1]    # choose maximum class index for the most predominant index\n",
    "                # pred: tensor([4, 3, 2, 4, 0, 3, 4, 3])\n",
    "                #pred = pred.cpu().detach()\n",
    "                pred = pred.cpu().detach().numpy()\n",
    "\n",
    "                # prepare to count predictions for each class\n",
    "                correct_pred = {classname: 0 for classname in w2v_classes}\n",
    "                total_pred = {classname: 0 for classname in w2v_classes}\n",
    "                # collect the correct predictions for each class\n",
    "                for label, prediction in zip(y, pred):\n",
    "                    if label == prediction:\n",
    "                        correct_pred[w2v_classes[label]] += 1\n",
    "                    total_pred[w2v_classes[label]] += 1   # {0: 3, 1: 14, 2: 20, 3: 2, 4: 1}\n",
    "                #print(\"y\", len(y))\n",
    "                #np.vstack((all_preds, pred))\n",
    "                #print(len(pred))   # 64, 64, 64, ...\n",
    "\n",
    "                # print accuracy for each class\n",
    "                #for classname, correct_count in correct_pred.items():\n",
    "                 #   accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "                    #print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname, accuracy))\n",
    "                  #  print(\"Accuracy for class {} is: {} %\".format(classname, accuracy))\n",
    "\n",
    "                #total_acc += accuracy_score(y.cpu(), pred.cpu() > 0.5)\n",
    "                total_acc += accuracy_score(y.cpu().detach().numpy(), np.hstack(pred))\n",
    "                #total_f1 += f1_score(y.cpu(), pred.cpu() > 0.5, average=None)\n",
    "                total_f1 += f1_score(y.cpu().detach().numpy(), np.hstack(pred), average='weighted')\n",
    "                \n",
    "                # save the predictions and the labels for each batch\n",
    "                #print(\"pred\", len(pred))\n",
    "                #print(\"pred type\", type(pred))\n",
    "                #print(\"all_preds\", type(all_preds))\n",
    "                all_preds = np.hstack([all_preds, pred])\n",
    "                #print(\"all_preds\", len(all_preds))\n",
    "\n",
    "            # calculate the average loss and average metrics\n",
    "            total_loss = total_loss / len(dataset)\n",
    "            total_acc = total_acc / len(dataset)\n",
    "            total_f1 = total_f1 / len(dataset)\n",
    "\n",
    "            # return the loss and print the calculated metrics\n",
    "            if mode:\n",
    "                print(\"Test: loss: {}, accuracy: {}%, f-score: {}\".format(total_loss, total_acc * 100, total_f1))\n",
    "            else:\n",
    "                print(\"Validation: loss: {}, accuracy: {}%, f-score: {}\".format(total_loss, total_acc * 100, total_f1))\n",
    "        \n",
    "        #print(len(pred))  # 10\n",
    "        t.enable_grad()\n",
    "        \n",
    "        if mode:\n",
    "            #print(len(all_preds))\n",
    "            return total_loss, all_preds\n",
    "        else:\n",
    "            return total_loss\n",
    "\n",
    "    def fit(self, n_epochs):\n",
    "        # to track the training loss as the model trains\n",
    "        #train_losses = []\n",
    "        # to track the validation loss as the model trains\n",
    "        #valid_losses = []\n",
    "        # to track the average training loss per epoch as the model trains\n",
    "        avg_train_losses = []\n",
    "        # to track the average validation loss per epoch as the model trains\n",
    "        avg_valid_losses = []\n",
    "        # store results\n",
    "        #res = open('./results/' + self.model_name + '_results.txt', 'w')\n",
    "        #res.write(50 * '=')\n",
    "        #res.write('Model \\n')\n",
    "        #res.write(str(self._model) + '\\n')\n",
    "\n",
    "        # load the last checkpoint with the best model\n",
    "        self.restore_checkpoint()\n",
    "\n",
    "        # initialize the early_stopping object\n",
    "        early_stopping = EarlyStopping(patience=self._early_stopping_patience, verbose=True)\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            # train the model\n",
    "            train_loss = self.train_epoch()\n",
    "            # validate the model\n",
    "            valid_loss = self.val_test(mode=False)\n",
    "\n",
    "            # calculate average loss over an epoch\n",
    "            #train_loss = np.average(train_losses)\n",
    "            #train_loss = train_losses / len(self._train_dl)\n",
    "            #valid_loss = np.average(valid_losses)\n",
    "\n",
    "            avg_train_losses.append(train_loss)\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "\n",
    "            \"\"\"\n",
    "            # print training/validation statistics\n",
    "            epoch_len = len(str(n_epochs))\n",
    "\n",
    "            print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                         f'train_loss: {train_loss:.5f} ' +\n",
    "                         f'valid_loss: {valid_loss:.5f}')\n",
    "\n",
    "            print(print_msg)\n",
    "            \"\"\"\n",
    "\n",
    "            # early_stopping needs the validation loss to check if it has decreased,\n",
    "            # if it has, it will make a checkpoint of the current model\n",
    "            early_stopping(valid_loss, self._model, self.model_name, self.dataset)\n",
    "\n",
    "            \"\"\"\n",
    "            # use the save_checkpoint function to save the model for each epoch\n",
    "            save_flag = self._early_stopping_cb.step(l_dev)\n",
    "\n",
    "            if save_flag:\n",
    "                res.write(50 * '=')\n",
    "                res.write('Epoch: ' + str(self.epoch) + ' Training Loss :' + str(l_train) + ' Development Loss :' + str(\n",
    "                    l_dev))\n",
    "                Trainer.save_checkpoint(self, self.epoch + 1, model_name)\n",
    "                self.epoch_n = self.epoch + 1\n",
    "            \"\"\"\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping has been reached\")\n",
    "                break\n",
    "\n",
    "            # load the last checkpoint with the best model\n",
    "            #self._model.load_state_dict(t.load('checkpoint.pt'))\n",
    "            #self.restore_checkpoint()\n",
    "\n",
    "        # return model, avg_train_losses, avg_valid_losses\n",
    "        #res.close()\n",
    "        return avg_train_losses, avg_valid_losses\n",
    "    \n",
    "    def test(self):\n",
    "\n",
    "        # load the last checkpoint with the best model\n",
    "        self.restore_checkpoint()\n",
    "\n",
    "        # validate the model\n",
    "        test_loss, all_preds = self.val_test(mode=True)\n",
    "        return test_loss, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8973642-b64e-4e08-bfe7-5468ca267965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    # https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, epoch=-1, verbose=False, delta=0, trace_func=print):\n",
    "    #def __init__(self, patience=7, epoch=0, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model, model_name, dataset):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, model_name, dataset)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"EarlyStopping counter is higher than patience\")\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, model_name, dataset)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, model_name, dataset):\n",
    "        \"\"\"\n",
    "        Saves model when validation loss decreases\n",
    "        \"\"\"\n",
    "        if not os.path.isdir('./checkpoints/'):\n",
    "            os.makedirs('./checkpoints/')\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        #a.save(model.state_dict(), self.path)\n",
    "        path = 'checkpoints/' + model_name + dataset + '_checkpoint_{}.ckp'.format(get_datetime())\n",
    "        t.save({'state_dict': model.state_dict()}, path)\n",
    "        self.val_loss_min = val_loss\n",
    "        #t.save({'state_dict': self._model.state_dict()}, 'checkpoints/' + model_name + 'checkpoint.ckp')\n",
    "\n",
    "\n",
    "def get_datetime():\n",
    "    #return strftime(\"%Y-%m-%d_%H:%M:%S\", gmtime())\n",
    "    # storing per day to have different runs from different days\n",
    "    return strftime(\"%Y-%m-%d\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65cc25f1-21c5-4513-b19d-c23b7e84e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53664f11-c569-4677-a9cb-00e045aa0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            #nn.Linear(3072, 1000),     # icp_ft.npy\n",
    "            nn.Linear(6144, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convert tensor (128, 1, 28, 28) --> (128, 1*28*28)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# defining model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim_1, hidden_dim_2, out_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim_1 = hidden_dim_1\n",
    "        self.hidden_dim_2 = hidden_dim_2\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        ## 1st hidden layer\n",
    "        self.linear_1 = nn.Linear(self.in_dim, self.hidden_dim_1)\n",
    "        self.linear_1.weight.detach().normal_(0.0, 0.1)\n",
    "        self.linear_1.bias.detach().zero_()\n",
    "        self.linear_1_bn = nn.BatchNorm1d(self.hidden_dim_1, momentum=0.6)\n",
    "\n",
    "        ## 2nd hidden layer\n",
    "        self.linear_2 = nn.Linear(self.hidden_dim_1, self.hidden_dim_2)\n",
    "        self.linear_2.weight.detach().normal_(0.0, 0.1)\n",
    "        self.linear_2.bias.detach().zero_()\n",
    "        self.linear_2_bn = nn.BatchNorm1d(self.hidden_dim_2, momentum=0.6)\n",
    "\n",
    "        ## Out layer\n",
    "        self.linear_out = nn.Linear(self.hidden_dim_2, self.out_dim)\n",
    "        self.linear_out.weight.detach().normal_(0.0, 0.1)\n",
    "        self.linear_out.bias.detach().zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear_1(x)\n",
    "        out = self.linear_1_bn(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.linear_2(out)\n",
    "        out = self.linear_2_bn(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=0.175, training=self.training)\n",
    "\n",
    "        out = self.linear_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf0e1b-ddde-4ec3-8f95-4ec05bd2a0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear_1): Linear(in_features=3072, out_features=2000, bias=True)\n",
      "  (linear_1_bn): BatchNorm1d(2000, eps=1e-05, momentum=0.6, affine=True, track_running_stats=True)\n",
      "  (linear_2): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "  (linear_2_bn): BatchNorm1d(1000, eps=1e-05, momentum=0.6, affine=True, track_running_stats=True)\n",
      "  (linear_out): Linear(in_features=1000, out_features=5, bias=True)\n",
      ")\n",
      "Train: loss: 2.3500090923309327\n",
      "Validation: loss: 1.9795119596852198, accuracy: 30.530753968253972%, f-score: 0.35944901362545745\n",
      "Validation loss decreased (inf --> 1.979512).  Saving model ...\n",
      "Train: loss: 1.8318559484481811\n",
      "Validation: loss: 1.7295352319876354, accuracy: 30.355076058201057%, f-score: 0.3520913281848417\n",
      "Validation loss decreased (1.979512 --> 1.729535).  Saving model ...\n",
      "Train: loss: 1.5919735860824584\n",
      "Validation: loss: 1.7456355061795976, accuracy: 31.485615079365083%, f-score: 0.36431099737964023\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Train: loss: 1.3869953410625457\n",
      "Validation: loss: 1.6772470672925313, accuracy: 34.26545965608466%, f-score: 0.3798495829730238\n",
      "Validation loss decreased (1.729535 --> 1.677247).  Saving model ...\n",
      "Train: loss: 1.2203494005203248\n",
      "Validation: loss: 1.641949666870965, accuracy: 35.00124007936508%, f-score: 0.3782188133864694\n",
      "Validation loss decreased (1.677247 --> 1.641950).  Saving model ...\n",
      "Train: loss: 1.0909791338443755\n",
      "Validation: loss: 1.676248096757465, accuracy: 36.47900132275132%, f-score: 0.4093242118300206\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Train: loss: 0.9772315466403961\n",
      "Validation: loss: 1.6951459480656519, accuracy: 33.876901455026456%, f-score: 0.3746545167244383\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Train: loss: 0.8933191473484039\n",
      "Validation: loss: 1.6798269483778212, accuracy: 43.47098214285714%, f-score: 0.46175564339672476\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Train: loss: 0.8241814098358154\n",
      "Validation: loss: 1.7050586177243128, accuracy: 35.385664682539684%, f-score: 0.3959252691158877\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Train: loss: 0.7915702786445618\n",
      "Validation: loss: 1.6809331509802077, accuracy: 31.834904100529098%, f-score: 0.356613225751529\n",
      "EarlyStopping counter: 5 out of 20\n"
     ]
    }
   ],
   "source": [
    "#model = MLP(6144, 3000, 1000, 7)\n",
    "model = MLP(3072, 2000, 1000, 5)\n",
    "print(model)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "# set up the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.1)\n",
    "\n",
    "trainer = Trainer(model, \"w2v\", \"aibo_icp\", criterion, optimizer, train_loader, val_loader, test_loader, \n",
    "                  cuda=t.cuda.is_available(),\\\n",
    "                  early_stopping_patience=20)\n",
    "\n",
    "# go, go, go... call fit on trainer\n",
    "res = trainer.fit(EPOCHS)\n",
    "\n",
    "# plot the results\n",
    "plt.plot(np.arange(len(res[0])), res[0], label='train loss')\n",
    "plt.plot(np.arange(len(res[1])), res[1], label='val loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('res/{}_losses_{}.png'.format(\"icp_aibo\", \"w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7fe66-6b18-4184-ad5c-9a5b3951e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574aa1eb-7f2c-4b11-8101-4ad6de33f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting testing\")\n",
    "test_res, pred = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5de2ad-5cbb-495f-8030-9c437d2ebf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.RdPu):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    # plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    #if normalize:\n",
    "    #    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "     #   print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "     #   print('Confusion matrix, without normalization')\n",
    "    \n",
    "    #print(title)\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('res/conf_matrix_{}.png'.format(\"aibo_to_icp\"))\n",
    "    plt.show()\n",
    "    return cm\n",
    "    \n",
    "def one_hot_encoder(true_labels, num_records, num_classes):\n",
    "    temp = np.array(true_labels[:num_records])\n",
    "    true_labels = np.zeros((num_records, num_classes))\n",
    "    true_labels[np.arange(num_records), temp] = 1\n",
    "    return true_labels\n",
    "\n",
    "def display_results(y_test, pred_probs):\n",
    "    #pred = np.argmax(pred_probs, axis=-1)\n",
    "    #one_hot_true = one_hot_encoder(y_test, len(pred), len(CLASS_TO_ID))\n",
    "    one_hot_true = one_hot_encoder(y_test, len(pred_probs), len(CLASS_TO_ID))\n",
    "    acc = 'Test Set Accuracy =  {0:.3f}'.format(accuracy_score(y_test, pred_probs))\n",
    "    fscore = 'Test Set F-score =  {0:.3f}'.format(f1_score(y_test, pred_probs, average='macro'))\n",
    "    prec = 'Test Set Precision =  {0:.3f}'.format(precision_score(y_test, pred_probs, average='macro'))\n",
    "    rec = 'Test Set Recall =  {0:.3f}'.format(recall_score(y_test, pred_probs, average='macro'))\n",
    "    metrics = [acc, fscore, prec, rec]\n",
    "    #if cm:\n",
    "    cr = classification_report(y_true=y_test, y_pred=pred_probs, target_names=list(ID_TO_CLASS.values()))\n",
    "    cm = plot_confusion_matrix(confusion_matrix(y_test, pred), classes=list(ID_TO_CLASS.values()))\n",
    "    f = open('res/report_{}.txt'.format('aibo_to_icp'), 'w')\n",
    "    f.write('RESULTS\\n\\nMetrics\\n\\n{}\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n'.format(metrics, cr, cm))\n",
    "    f.close()\n",
    "    print(cr)\n",
    "    plot_confusion_matrix(confusion_matrix(y_test, pred), classes=list(ID_TO_CLASS.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f98f5bbe-1669-47cf-a7d3-ace49cd75fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "701f2e3c-98b4-4394-b2d1-b4d1dc535a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46c1c8c9-c26f-41ed-9c42-a5ea42340a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neu', 'hap', 'sad', 'fru', 'ang']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0839c39f-9c45-4a0c-9980-d1948f5ef474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, ..., 3, 3, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de530f6d-1de2-4dd6-ac42-220b77a3a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408daa7-40ed-4491-9294-2da5657340af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "779cb7e5-3e88-4737-9fd2-4801aa2ddbff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Matching AIBO labels to ICP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e8769-c05d-49da-b217-78e16578e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.vstack((y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187ef56-5745-486e-99f7-d299c6b9a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71387630-a1d3-421e-a18d-904b175d2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af777015-8dcf-4c4e-bedb-3684dea2af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093a43a-dbf0-49e3-8710-594a3e84d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40d078-c773-4bf6-b2c0-1b7813153e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIBO_CLASS_TO_ID = {'A': 0, 'E': 1, 'N': 2, 'P': 3, 'R': 4}\n",
    "AIBO_ID_TO_CLASS = {v: k for k, v in AIBO_CLASS_TO_ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ccd846-48c2-4c95-81ed-ad68b8cae21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_icp = [AIBO_ID_TO_CLASS[pred[i]] for i in range(len(pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2893ee-1056-4665-a6a1-485dd6c36762",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_icp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87092d3d-2656-4dfc-803d-2f2b8d74d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_icp[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc6237-54ec-4b9a-8cd2-7ff5963ecfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICP_CLASS_TO_ID = {\"neu\": 0, \"hap\": 1, \"sur\": 2, \"sad\": 3, \"fru\": 4, \"ang\": 5, \"fea\": 6}\n",
    "ICP_ID_TO_CLASS = {v: k for k, v in ICP_CLASS_TO_ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9451f-3886-4027-bdde-cbeee075e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_aibo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0335c27-b7be-4d1d-a667-b27fa7d4c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_aibo = [ICP_ID_TO_CLASS[labels[i]] for i in range(len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d57395-bf32-4f01-bd10-5005c9b4e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(labels_aibo, pred_icp)), columns=['labels_aibo', 'pred_icp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732845a-7a97-4cd4-9cf2-e10c3db0a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.labels_aibo.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfceaf-7f2f-465f-b88e-47486faf6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_mapping(emotion):\n",
    "    filt_df = df[df.labels_aibo == emotion]\n",
    "    print(filt_df.shape)\n",
    "    \n",
    "    d = filt_df.pred_icp.value_counts().to_dict()\n",
    "    #s = sum(d.values())\n",
    "    perc = []\n",
    "    for k, v in d.items():\n",
    "        pct = v * 100.0 / sum(d.values())\n",
    "        perc.append(pct)\n",
    "    p = dict(zip(d.keys(), perc))\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = [10, 7]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    x = d.keys()\n",
    "    y = d.values()\n",
    "    percentage = list(p.values())\n",
    "    #ax = sns.barplot(x=x, y=y, palette='PuBuGn_r')\n",
    "\n",
    "    width = 0.35\n",
    "    fig, ax = plt.subplots()\n",
    "    pps = ax.bar(x, y, width, align='center', color ='maroon',)\n",
    "    patches = ax.patches\n",
    "    for i in range(len(patches)):\n",
    "       x = patches[i].get_x() + patches[i].get_width()/2\n",
    "       y = patches[i].get_height()+.05\n",
    "       ax.annotate('{:.1f}%'.format(percentage[i]), (x, y), ha='center')\n",
    "\n",
    "    plt.xlabel(\"ICP Labels\")\n",
    "    plt.ylabel(\"Occurences\")\n",
    "    plt.title(\"AIBO '{}' Emotion Label in IEMOCAP label terms\".format(emotion))\n",
    "    plt.savefig('res/aibo_matching_{}.png'.format(emotion))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a216895-6bd6-4183-a916-00a69ff70f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emotion_mapping(emotion='sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0f4fb-693e-40c1-a94c-454a378fdb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emotion_mapping(emotion='hap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac9703-5f4d-47e8-847d-ad9ad8abf488",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emotion_mapping(emotion='fru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70d0eb-bf71-4859-8338-291816948ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emotion_mapping(emotion='neu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1c669-a42a-4156-8c11-0808d8d76d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emotion_mapping(emotion='sur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94d2d0-1855-4689-86dc-a620b0886fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd603eca-b09a-491e-aae1-7109589da3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166be031-c31f-46c9-92af-02aa681ada28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05285fe2-cad9-4d34-a47b-7c197429e779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70155ca-51a4-4259-aa2e-0d318df5c5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc9c0f-2080-4d53-a012-b889a6be44db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env:Python",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
